# AI Governance Baseline – Mapping Grid (v1)

| Theme                  | EU AI Act Requirement(s) | NIST AI RMF Function(s) | Control Objective | Controls (Preventive / Detective / Corrective) | Evidence Examples |
|-------------------------|--------------------------|--------------------------|------------------|-----------------------------------------------|------------------|
| **1. Data governance** | Art. 10 (data quality, governance) | Map, Measure | Ensure training data is documented, high-quality, and traceable | **Preventive:** Require data sources to be registered before ingestion. <br> **Detective:** Run automated data quality checks (missing values, duplicates). <br> **Corrective:** Reject or quarantine datasets that fail validation. | Data dictionary, provenance logs, validation reports |
| **2. Model inventory & classification** | Art. 52 (transparency), Annex III (high-risk systems) | Govern, Map | Maintain up-to-date register of AI models with risk tier | **Preventive:** Enforce mandatory model registration before deployment. <br> **Detective:** Quarterly audit of deployed vs. registered models. <br> **Corrective:** Block or retire unregistered models. | Model catalog, risk rating sheet, audit reports |
| **3. Security (IAM, logging, monitoring)** | Art. 15 (robustness, cybersecurity) | Manage | Protect AI infra with strong access, logging, and monitoring | **Preventive:** Enforce MFA + least-privilege IAM roles. <br> **Detective:** Continuous log monitoring for anomalous access. <br> **Corrective:** Auto-revoke keys/sessions when anomalies found. | IAM policies, CloudTrail logs, SIEM alerts |
| **4. Privacy & data protection** | GDPR alignment + Art. 10 (PII minimization) | Govern, Map | Protect personal data used in AI lifecycle | **Preventive:** Data minimization rules + pseudonymization. <br> **Detective:** Regular DPIAs and privacy impact scans. <br> **Corrective:** Purge sensitive data when noncompliance detected. | DPIA, anonymization logs, retention schedules |
| **5. Human oversight & accountability** | Art. 14 (human oversight) | Govern, Manage | Ensure humans can review, override, or escalate AI outputs | **Preventive:** Define approval workflows for AI decisions. <br> **Detective:** Random sampling of AI outputs for human review. <br> **Corrective:** Override button/escalation hotline for exceptions. | SOPs, approval logs, escalation records |
| **6. Testing & evaluation** | Art. 15 (robustness, accuracy, bias testing) | Measure, Manage | Regularly test AI systems for accuracy, bias, and robustness | **Preventive:** Require pre-deployment evaluation reports. <br> **Detective:** Scheduled re-testing for bias/adversarial robustness. <br> **Corrective:** Retrain or disable models that fail thresholds. | Test scripts, evaluation metrics, robustness reports |
| **7. Incident management & response** | Art. 9 (risk mgmt), Art. 62 (post-market monitoring) | Manage | Detect, respond, and learn from AI-related incidents | **Preventive:** Maintain AI-specific incident response plan. <br> **Detective:** AI monitoring dashboard for drift/failures. <br> **Corrective:** Root-cause analysis and remediation after incident. | Incident playbook, RCA reports, postmortems |
| **8. Third-party/vendor risk** | Art. 28–30 (provider obligations, supply chain) | Govern, Manage | Assess and monitor AI vendors/partners | **Preventive:** Perform vendor AI risk assessment before onboarding. <br> **Detective:** Annual reassessments of vendor AI controls. <br> **Corrective:** Terminate/replace vendors that fail obligations. | VRA results, contracts, SLA monitoring reports |


